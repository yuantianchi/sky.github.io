---
layout:     post
title:      爬虫实践指南（一）
subtitle:   爬虫笔记
date:       2019-03-27
author:     Yuantc
header-img: img/in-post/post-bg-os-metro.jpg
catalog: true
tags:
    - 爬虫
---

#需基本的技能
熟悉python开发，熟悉http协议，了解前端基础（网页的基本组成等）

# Python爬虫架构
Python 爬虫架构主要由五个部分组成，分别是调度器、URL管理器、网页下载器、网页解析器、应用程序（爬取的有价值数据）。
**调度器：** 相当于一台电脑的CPU，主要负责统筹其他四个模块（URL管理器、下载器、解析器之间的协调工作）的协调工作调度。

**URL管理器：** 管理URL链接，维持已经爬取的URL集合和未爬取的URL集合，提供新的URL链接接口。实现URL管理器主要用三种方式，通过内存、队列、数据库、缓存数据库来实现。

**网页下载器：** 通过传入一个URL地址来下载网页。网页下载器有requests、urllib2、urllib包括需要登录、代理、和cookie

**网页解析器：** 将下载的网页内容进行解析，提取出我们有用的信息。常见解析器有：正则表达式（当文档比较复杂的时候，该方法提取数据的时候就会非常的困难）、html.parser（Python自带的）、lxml（第三方插件，可以解析 xml 和 HTML）、
beautifulsoup（第三方插件，可以使用Python自带的html.parser进行解析，也可以使用lxml进行解析，相对于其他几种来说要强大一些）、XPath（是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。）
html.parser 和 beautifulsoup 以及 lxml 都是以 DOM 树的方式进行解析的。

**数据存储器：** 用于将HTML解析器解析出来的数据通过文件或者数据库形式储存起来。常见的写入到文本如 txt，csv，excel 等，保存到数据库如 sqlite、MySQL、mongodb等

往往爬虫在解析页面中对数据清洗比较麻烦，所以我们简单了解网页解析方面的知识

# BeautifulSoup的基本使用语法规则
.find() 使用示例
soup.find('a')。那么会返回在soup包含的源代码中，遇到的第一个<a>...</a>标签内容对象。
soup.find('a', id='next')。那么会返回在soup包含的源代码中，遇到的第一个有属性为id，值为next的<a>对象，比如<a id="next">...</a>。(不只可以用id，大部分其他的属性都可以直接使用，比如src、name。 值得注意的是，class这个属性因为是Python关键字，不能直接使用，所以在BS里面，使用class_='...'进行代替 )
find返回的结果，依然可以继续使用find()或者find_all()方法。如果找不到指定的内容，find会返回None。
.find_all()使用示例
soup.find_all('a')。那么会返回在soup包含的源代码中，遇到的所有<a>...</a>标签内容的可迭代对象(我们可以把它看成一个 list 或者数组)。
soup.find_all('a', class_='next')。那么会返回在soup包含的源代码中，遇到的所有属性为class，值为next的<a>的 可迭代对象，比如<a class="next">...</a>。(语法和find也一样，class也不能直接写)
find_all返回的“list”中的单个对象 依然可以继续使用find()或者find_all()方法。如果找不到指定的内容，find_all会返回一个空的“list”。
获取元素的某个属性
soup['src]，这样我们就能取出soup对象的src属性了。如果该属性不存在，那么程序会报错。
获取元素中的所有文本soup.text，假设soup对象为<div>你好<a>复联</a></div>，那么这个操作返回字符串是你好复联。
附上BeautifulSoup 的文档：
中文文档：[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)

# 简易爬虫示例

本章我们会以 爬取豆瓣电影成都即将上映的影片信息 作为案例讲解 BeautifulSoup 的用法。

我们需要爬取的内容有：所有影片的 名字、详情链接、上映时间、影片类型、地区、关注者数量、影片简介。

网页截图如下：
![avatar](https://yuantianchi.github.io/posts_image/pachong/pachong_1.png)

## 步骤一 分析网页
分析网页，制订提取内容策略
这一步非常重要，直接影响了我们能不能提取到我们想要的内容。
我们返回浏览器打开的豆瓣网页。例如：找到网页中的第一个电影的名字，鼠标指向该名字，点击右键，选择 检查/审查元素，然后便会打开一个新的小窗口在页面上，并且将网页代码中电影的名字显示在了里面，并且你鼠标指向的元素会显示出它的大小，内容会被选中。


![avatar](https://yuantianchi.github.io/posts_image/pachong/pachong_2.png)

我们同时滑动鼠标的位置，应该会发现    

当鼠标划到图片中的 **\<ul>...\</ul>** 标签的时候，复仇者联盟影片的详细信息被选中了。

当鼠标划到下一个 **\<div class="item mod odd">...\</div>** 的时候，下一个影片战犬瑞克斯的所有信息被选中了。

当鼠标划到图片上方的 **\<div id="showing-soon" class="tab-bd">** 的时候，整个我们需要采集的影片信息都被选中了。

当点击电影名时跳转到详情页中，接着点击剧情简介时 **\<div class="indent" id="link-report">...\</div>** 中的**\<span>...</span>** 
标签被选中


点击这几个动作告诉了我们的信息有：

我们需要的内容全都在 **\<div id="showing-soon" class="tab-bd">\</div>** 里面。

每个影片的信息，都在一个 **\<div class="item mod odd">...\</div>** 或者 **\<div class="item mod">...\</div>** 里面。

画面左边的影片没有odd属性，右边的有odd属性(这好像对于我们采集信息没啥用)。

剧情简介在详情链接对应的页面里在一个 **\<div class="indent" id="link-report">...\</div>** 的 **\<span>...</span>** 标签中


| 电影属性 | 文档中的位置 |
| :----: | :----:| 
| 名字 | 在第 2 个<a>标签里面 |
| 链接 | 在第 1 个和第 2 个 \<a> 标签的 href 属性里面 |
| 上映日期 | 在第 1 个 \<li> 标签里面 |
| 类型 | 在第 2 个 \<li> 标签里面 |
| 地区 | 在第 3 个 \<li> 标签里面 |
| 关注者数量 | 在第 4 个 \<li> 标签里面 |
| 剧情简介 | 链接对应页面id 为link-report 的 \<span>标签中|


## 步骤二 制定策略
那么我们的策略，就是先找到囊括了所有的影片的div，然后再从这个div里面找到所有的影片的div，最后再从每个影片的div里面解析出来我们需要的名字、链接等等信息，获取链接信息后
再找到id为link-report的div中的span标签的类容



## 步骤三 编写代码如下:

    # ！/usr/bin/env python
    # ! -*- coding:utf-8 -*-
    import requests
    from bs4 import BeautifulSoup  # 从bs4引入BeautifulSoup
    
    
    class DouBanCrawler(object):
        def __init__(self):
            pass
    
        # 下载器，下载页面
        def htmlDownloader(self, url, page_name):
            response = requests.get(url)
            file_obj = open(page_name, 'w', encoding="utf-8")
            file_obj.write(response.content.decode('utf-8'))
            file_obj.close()
    
        # 读取页面信息
        def readPage(self, page_name):
            file_obj = open(page_name, 'r', encoding='utf-8')  # 以读方式打开文件名为douban.html的文件
            html = file_obj.read()  # 把文件的内容全部读取出来并赋值给html变量
            file_obj.close()  # 关闭文件对象
            return html
    
    
    if __name__ == '__main__':
        T = DouBanCrawler()
        url="https://movie.douban.com/cinema/later/wuhan/"
        T.htmlDownloader(url, 'douban1.html')
        # 读取文件内容到html变量里面
        html = T.readPage('douban1.html')
        soup = BeautifulSoup(html, 'lxml')  # 初始化BeautifulSoup
    
        all_movies = soup.find('div', id='showing-soon')
        href_list = []
        for each_movies in all_movies.find_all('div', class_='item'):
            all_a_tag = each_movies.find_all('a')  # 找到所有的a标签
            all_li_tag = each_movies.find_all('li')  # 找到所有的li标签
            movie_name = all_a_tag[1].text
            movie_href = all_a_tag[1]['href']
            movie_date = all_li_tag[0].text  # 从第1个li标签的文字内容提取影片上映时间
            movie_type = all_li_tag[1].text
            movie_area = all_li_tag[2].text
            movie_lovers = all_li_tag[3].text
    
            T.htmlDownloader(movie_href, 'href.html')
            href_html = T.readPage('href.html')
            soup2 = BeautifulSoup(href_html, 'lxml')  # 初始化BeautifulSoup
            href_link_text = soup2.find('div', id='link-report').find('span').text
    
            print('电影名称：{}，链接：{}，日期：{}，类型：{}，地区：{}， 关注者：{}, 剧情简介：{}'.format(movie_name, movie_href, movie_date, movie_type, movie_area, movie_lovers, href_link_text))


